# mro_agenticops

**AgenticOps | LLMOps + RAG Local | Offline-First Architecture**

Sistema de RAG (Retrieval-Augmented Generation) para documentos MRO (Modelo de Responsabilidade Organizacional) com execuÃ§Ã£o 100% local usando Ollama + FAISS.

---

## ğŸ—ï¸ Arquitetura

### Camadas do Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CAMADA AGENTIC (Futuro)                  â”‚
â”‚  Agentes autÃ´nomos | Planejamento | Ferramentas | MemÃ³ria   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CAMADA DE APLICAÃ‡ÃƒO                        â”‚
â”‚  src/query.py    â”‚ src/agent.py  â”‚  prompts/                â”‚
â”‚  OrquestraÃ§Ã£o RAG â”‚ LÃ³gica agentic â”‚ Templates              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 CAMADA DE RETRIEVAL                         â”‚
â”‚  src/retriever.py  â”‚  src/embeddings.py â”‚ src/rerank.py    â”‚
â”‚  FAISS index       â”‚  Sentence-BERT     â”‚ Reranking        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CAMADA DE GERAÃ‡ÃƒO                         â”‚
â”‚      src/llm.py    â”‚    Ollama API     â”‚   Context Manager â”‚
â”‚      Llama 3.x     â”‚    Local Server   â”‚   Prompt Builder  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CAMADA DE DADOS                            â”‚
â”‚  src/preprocess.py â”‚ src/chunking.py â”‚ data/raw/*.pdf      â”‚
â”‚  PDF extraction    â”‚ Semantic chunks â”‚ Metadata enrichment â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes Principais

#### 1. **Data Pipeline** (`src/preprocess.py`, `src/train.py`)
- **IngestÃ£o**: PyPDF2/pypdfium2 para extraÃ§Ã£o de PDFs
- **Chunking**: Semantic chunking com sobreposiÃ§Ã£o
- **Embeddings**: sentence-transformers (all-MiniLM-L6-v2)
- **IndexaÃ§Ã£o**: FAISS com Ã­ndice IVF ou HNSW para escala

#### 2. **Retrieval Engine** (`src/retriever.py`)
- **Vector Store**: FAISS (offline, rÃ¡pido)
- **EstratÃ©gia**: HÃ­brida (semantic + keyword BM25)
- **Reranking**: Cross-encoder para top-k refinement

#### 3. **LLM Integration** (`src/llm.py`)
- **Ollama API**: Cliente Python para modelos locais
- **Modelos suportados**: llama3, mistral, mixtral
- **Streaming**: Respostas progressivas para UX

#### 4. **Observability** (`monitoring/`)
- **MLflow**: Tracking de experimentos e mÃ©tricas RAG
- **Logs estruturados**: JSON para anÃ¡lise
- **MÃ©tricas**: LatÃªncia, relevÃ¢ncia, custo de contexto

---

## ğŸ“ Estrutura de DiretÃ³rios

```
mro_agenticops/
â”œâ”€â”€ src/                      # CÃ³digo fonte modular
â”‚   â”œâ”€â”€ preprocess.py         # Pipeline de ingestÃ£o de PDFs
â”‚   â”œâ”€â”€ train.py              # GeraÃ§Ã£o de embeddings + Ã­ndice FAISS
â”‚   â”œâ”€â”€ evaluate.py           # MÃ©tricas RAG (MRR, NDCG, recall)
â”‚   â”œâ”€â”€ retriever.py          # (A criar) Motor de busca vetorial
â”‚   â”œâ”€â”€ llm.py                # (A criar) Cliente Ollama
â”‚   â”œâ”€â”€ query.py              # (A criar) OrquestraÃ§Ã£o RAG end-to-end
â”‚   â””â”€â”€ agent.py              # (A criar) LÃ³gica agentic futura
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # PDFs originais
â”‚   â”œâ”€â”€ interim/              # Chunks processados (JSON/Parquet)
â”‚   â””â”€â”€ processed/            # FAISS index + metadados
â”œâ”€â”€ pipelines/                # Scripts de orquestraÃ§Ã£o
â”‚   â”œâ”€â”€ ingest_pipeline.py    # AutomaÃ§Ã£o preprocess â†’ train
â”‚   â””â”€â”€ eval_pipeline.py      # AutomaÃ§Ã£o de avaliaÃ§Ã£o contÃ­nua
â”œâ”€â”€ prompts/                  # Templates de prompts versionados
â”‚   â”œâ”€â”€ system_prompt.txt     # Contexto do sistema
â”‚   â””â”€â”€ rag_template.txt      # Template para RAG
â”œâ”€â”€ infra/                    # Scripts de setup local
â”‚   â”œâ”€â”€ setup_ollama.sh       # Install e pull de modelos
â”‚   â””â”€â”€ docker-compose.yml    # (Opcional) Ollama containerizado
â”œâ”€â”€ monitoring/               # Observabilidade
â”‚   â”œâ”€â”€ mlruns/               # MLflow experiments
â”‚   â””â”€â”€ logs/                 # Logs estruturados
â”œâ”€â”€ governance/               # Compliance e auditoria
â”‚   â””â”€â”€ data_policy.md        # PolÃ­ticas de uso de dados
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml          # ConfiguraÃ§Ãµes centralizadas
â”œâ”€â”€ tests/                    # Testes unitÃ¡rios e integraÃ§Ã£o
â”œâ”€â”€ .github/workflows/        # CI/CD (opcional)
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ InÃ­cio RÃ¡pido

### 1. Configurar Ambiente

```bash
# Criar ambiente conda
conda create -n mro python=3.11 -y
conda activate mro

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 2. Configurar Ollama

```bash
# Iniciar servidor (terminal separado)
ollama serve

# Baixar modelo (ex: llama3)
ollama pull llama3
```

### 3. Processar Dados

```bash
# ExtraÃ§Ã£o e chunking de PDFs
python src/preprocess.py

# Gerar embeddings e Ã­ndice FAISS
python src/train.py
```

### 4. Executar RAG

```python
from src.query import RAGPipeline

rag = RAGPipeline(config_path="configs/default.yaml")
resposta = rag.query("O que Ã© o Modelo de Responsabilidade Organizacional?")
print(resposta)
```

---

## ğŸ“Š Observability & Evaluation

### MÃ©tricas RAG

- **Retrieval Metrics**: MRR@10, Recall@5, NDCG@10
- **Generation Metrics**: BLEU, ROUGE, BERTScore
- **LatÃªncia**: p50, p95, p99 para retrieval e geraÃ§Ã£o
- **Contexto**: Tokens consumidos, chunk relevance score

### MLflow Tracking

```python
import mlflow

mlflow.set_experiment("mro_rag_v1")
with mlflow.start_run():
    mlflow.log_param("model", "llama3")
    mlflow.log_metric("recall@5", 0.87)
```

---

## ğŸ”® Roadmap para EvoluÃ§Ã£o Agentic

### Fase 1: RAG BÃ¡sico (Atual)
- [x] IngestÃ£o de PDFs
- [x] Embeddings + FAISS
- [ ] Query pipeline end-to-end
- [ ] AvaliaÃ§Ã£o automatizada

### Fase 2: RAG AvanÃ§ado
- [ ] Reranking com cross-encoder
- [ ] Hybrid search (BM25 + vetorial)
- [ ] Query decomposition
- [ ] Citation tracking

### Fase 3: Agentic RAG
- [ ] ReAct agent com ferramentas
- [ ] Planejamento multi-step
- [ ] MemÃ³ria de conversaÃ§Ã£o
- [ ] Self-correction loops

### Fase 4: Multi-Agent System
- [ ] Agente "Pesquisador" (retrieval specialist)
- [ ] Agente "Analista" (synthesis)
- [ ] Agente "Validador" (fact-checking)
- [ ] Orquestrador central

---

## âš™ï¸ ConfiguraÃ§Ã£o

Edite `configs/default.yaml`:

```yaml
# LLM
ollama:
  base_url: http://localhost:11434
  model: llama3
  temperature: 0.1
  max_tokens: 2048

# Retrieval
retriever:
  top_k: 5
  similarity_threshold: 0.7
  rerank: true

# Embeddings
embeddings:
  model: sentence-transformers/all-MiniLM-L6-v2
  device: cpu  # ou 'cuda'

# Chunking
chunking:
  chunk_size: 512
  chunk_overlap: 50
```

---

## ğŸ§ª Testes

```bash
pytest tests/ -v
```

---

## ğŸ“ ContribuiÃ§Ã£o

1. Criar branch: `git checkout -b feature/nome`
2. Implementar com testes
3. Documentar mudanÃ§as
4. Pull Request para revisÃ£o

---

## ğŸ“š ReferÃªncias

- [Ollama API Docs](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [FAISS Documentation](https://faiss.ai/)
- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)
- [AgenticOps Patterns](https://www.anthropic.com/index/claude-2-1-prompting)

---

## ğŸ“„ LicenÃ§a

MIT License - Veja LICENSE para detalhes.
